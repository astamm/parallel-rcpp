[
  {
    "objectID": "slides/slides.html#information-about-the-session",
    "href": "slides/slides.html#information-about-the-session",
    "title": "Parallel computing with C++ from R",
    "section": "Information about the session",
    "text": "Information about the session\n\n\n\n\n\n\nCode copy-pasting\n\n\nAll pieces of code can be copy-pasted into an R session: to do that, just hover over the code and click on the “Copy” button that appears on the top right corner of the code block.\nR code. Copy-paste into an R script and run.\nC++ code. Copy-paste:\n\neither into a XXX.cpp file and compiled with Rcpp::sourceCpp(file = 'XXX.cpp');\nor into the code = '' argument of Rcpp::sourceCpp() function."
  },
  {
    "objectID": "slides/slides.html#benchmarkings",
    "href": "slides/slides.html#benchmarkings",
    "title": "Parallel computing with C++ from R",
    "section": "Benchmarkings",
    "text": "Benchmarkings\n\n\n\n\n\n\nBenchmarking\n\n\nAll time benchmarks are done using the {tictoc} package and on a MacBook Pro 2021 with an Apple M1 Pro chip including 10 cores and 32 GB of RAM under Sonoma 14.5 macOS."
  },
  {
    "objectID": "slides/slides.html#running-multi-threaded-code-in-c",
    "href": "slides/slides.html#running-multi-threaded-code-in-c",
    "title": "Parallel computing with C++ from R",
    "section": "Running multi-threaded code in C++",
    "text": "Running multi-threaded code in C++\n\nOpenMP (Dagum and Menon 1998)\nIntel Thread Building Blocks (TBB) (Reinders 2007)\nBoost.Thread (Schäling 2014)\nTinyThread++\nstd::thread (C++11)\nstd::jthread (C++20)"
  },
  {
    "objectID": "slides/slides.html#calling-multi-threaded-c-code-from-rcpp",
    "href": "slides/slides.html#calling-multi-threaded-c-code-from-rcpp",
    "title": "Parallel computing with C++ from R",
    "section": "Calling multi-threaded C++ code from R(cpp)",
    "text": "Calling multi-threaded C++ code from R(cpp)\n\nOpenMP (Dagum and Menon 1998)\nRcppParallel\nRcppThread (Nagler 2021) and the API documentation at https://tnagler.github.io/RcppThread/namespaceRcppThread.html."
  },
  {
    "objectID": "slides/slides.html#overview",
    "href": "slides/slides.html#overview",
    "title": "Parallel computing with C++ from R",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\nPreliminary remarks\n\n\n\nThis section is built from Matteo Fasiolo’s online course.\nWe assume basic familiarity with OpenMP (“Beginning OpenMP”; “OpenMP API Reference Guide”; “OpenMP Reference Sheet for C/C++,” n.d. for a refresher).\n\n\n\n\n\n\n\n\n\n\nThe magic of OpenMP\n\n\n\nOpenMP is a set of compiler directives, library routines, and environment variables that influence the behavior of parallelized code.\nIt is supported by most compilers, including g++, clang, and icc.\nIt is a simple and effective way to parallelize code: you start by adding a few directives to your existing sequential code, and the compiler does the rest."
  },
  {
    "objectID": "slides/slides.html#enabling-openmp",
    "href": "slides/slides.html#enabling-openmp",
    "title": "Parallel computing with C++ from R",
    "section": "Enabling OpenMP",
    "text": "Enabling OpenMP\n\n\n\n\n\n\nOpenMP support on Windows and Linux\n\n\n\nOn Windows and Linux, OpenMP is supported by default in g++ and clang.\nTo enable OpenMP in g++, use the -fopenmp flag.\nTo enable OpenMP in clang, use the -Xclang -fopenmp flags.\nUse Rcpp::plugins(openmp) in your C++ code to take care of the flags automagically.\nSummary: you should have nothing to do on Windows and Linux."
  },
  {
    "objectID": "slides/slides.html#enabling-openmp-1",
    "href": "slides/slides.html#enabling-openmp-1",
    "title": "Parallel computing with C++ from R",
    "section": "Enabling OpenMP",
    "text": "Enabling OpenMP\n\n\n\n\n\n\nOpenMP support on macOS\n\n\nApple has explicitly disabled OpenMP support in compilers that they ship in Xcode:\n  $ clang -c omp.c -fopenmp\n  clang: error: unsupported option '-fopenmp'\neven though clang had OpenMP support for quite a long time now. In fact, the clang compiler in Xcode can generate all the necessary code for OpenMP. It can be tricked into performing its designed function by using -Xclang -fopenmp flags.\nThe unfortunate part about this is that Apple is not shipping the necessary libomp.dylib run-time library needed for OpenMP support. Fortunately, some clever folks made them available for us: see &lt;https://mac.r-project.org/openmp/."
  },
  {
    "objectID": "slides/slides.html#a-toy-function-to-play-with",
    "href": "slides/slides.html#a-toy-function-to-play-with",
    "title": "Parallel computing with C++ from R",
    "section": "A toy function to play with",
    "text": "A toy function to play with\n\n//---------------------------------\n1#include &lt;unistd.h&gt;\n#include &lt;Rcpp.h&gt;\n\n2// [[Rcpp::export]]\nbool wait_k_seconds(unsigned int sec)\n{\n    for (unsigned int i = 0;i &lt; sec;++i)\n        sleep(1);\n    \n    return EXIT_SUCCESS;\n}\n\n\n1\n\nThe unistd.h header file is needed for the sleep() function.\n\n2\n\nThe [[Rcpp::export]] attribute tells Rcpp that it should generate the necessary R bindings for the function so that it can be called from R.\n\n\n\n\n\nThe previous code defines a simple function that waits for sec seconds and makes it available to R when compiled using Rcpp::sourceCpp(). We can test that it does what it is supposed to do by calling it from R:\n\n\nsystem.time(wait_k_seconds(2))[3]\n\nelapsed \n  2.119"
  },
  {
    "objectID": "slides/slides.html#a-parallelized-version-via-openmp",
    "href": "slides/slides.html#a-parallelized-version-via-openmp",
    "title": "Parallel computing with C++ from R",
    "section": "A parallelized version via OpenMP",
    "text": "A parallelized version via OpenMP\n\n//---------------------------------\n#include &lt;unistd.h&gt;\n#include &lt;Rcpp.h&gt;\n\n1// // [[Rcpp::plugins(openmp)]]\n\n// [[Rcpp::export]]\nbool wait_k_seconds_omp(unsigned int sec, unsigned int ncores)\n{\n2#if defined(_OPENMP)\n3    #pragma omp parallel num_threads(ncores)\n4    #pragma omp for\n#endif\n    for (unsigned int i = 0;i &lt; sec;++i)\n        sleep(1);\n    \n    return EXIT_SUCCESS;\n}\n\n\n1\n\nIncludes the correct OpenMP flags during compilation. Must not be included on macOS as OpenMP flags are handled in ~/.R/Makevars.\n\n2\n\nChecks if OpenMP is available and inserts the following code only if it is.\n\n3\n\nIndicates the beginning of a parallel section, to be executed on ncores parallel threads.\n\n4\n\nTells the compiler that the for loop should be run in parallel."
  },
  {
    "objectID": "slides/slides.html#benchmarking-1",
    "href": "slides/slides.html#benchmarking-1",
    "title": "Parallel computing with C++ from R",
    "section": "Benchmarking",
    "text": "Benchmarking\n\nRun a 3 sec task on 1 thread:\n\n\nsystem.time(wait_k_seconds_omp(3, 1))[3]\n\nelapsed \n  3.293 \n\n\n\nRun a 3 sec task on 3 threads:\n\n\nsystem.time(wait_k_seconds_omp(3, 3))[3]\n\nelapsed \n  1.092"
  },
  {
    "objectID": "slides/slides.html#a-more-realistic-example-r",
    "href": "slides/slides.html#a-more-realistic-example-r",
    "title": "Parallel computing with C++ from R",
    "section": "A more realistic example (R)",
    "text": "A more realistic example (R)\nSay we want to check if all elements of a numeric vector are finite. We can write a first naive function in R:\n\nall_finite_r_v1 &lt;- function(x) {\n  all(is.finite(x))\n}\n\nWe can improve upon this version by summing all elements of the vector and checking if the result is finite:\n\nall_finite_r_v2 &lt;- function(x) {\n  is.finite(sum(x))\n}"
  },
  {
    "objectID": "slides/slides.html#a-c-version",
    "href": "slides/slides.html#a-c-version",
    "title": "Parallel computing with C++ from R",
    "section": "A C++ version",
    "text": "A C++ version\n\n//---------------------------------\n#include &lt;Rcpp.h&gt;\n\n// // [[Rcpp::plugins(openmp)]] // Uncomment on Windows and Linux\n\n// [[Rcpp::export]]\n1bool all_finite_cpp(Rcpp::NumericVector x)\n{\n  unsigned int nbInputs = x.size();\n\n  double out = 0;\n  for (unsigned int i = 0;i &lt; nbInputs;++i)\n    out += x[i];\n\n2  return R_FINITE(out);\n}\n\n\n1\n\nThe function takes a Rcpp::NumericVector as input. This is used because Rcpp automatically converts between R vectors and C++ vectors which is why we can pass an R vector directly to the function that sourceCpp generates.\n\n2\n\nThe function returns R_FINITE(out) which is a macro from the C API of R that checks if out is finite."
  },
  {
    "objectID": "slides/slides.html#parallelizing-via-openmp",
    "href": "slides/slides.html#parallelizing-via-openmp",
    "title": "Parallel computing with C++ from R",
    "section": "Parallelizing via OpenMP",
    "text": "Parallelizing via OpenMP\n\n//---------------------------------\n#include &lt;Rcpp.h&gt;\n\n// // [[Rcpp::plugins(openmp)]] // Uncomment on Windows and Linux\n\n// [[Rcpp::export]]\n1bool all_finite_omp(Rcpp::NumericVector x, unsigned int ncores)\n{\n  unsigned int nbInputs = x.size();\n  double out = 0;\n\n#ifdef _OPENMP\n2#pragma omp parallel for reduction(+:out) num_threads(ncores)\n#endif\n  for (unsigned int i = 0;i &lt; nbInputs;++i)\n    out += x[i];\n\n  return R_FINITE(out);\n}\n\n\n1\n\nThe function takes a Rcpp::NumericVector as input and an additional argument ncores which specifies the number of threads to use.\n\n2\n\nThe reduction(+:out) clause tells the compiler that the out variable should be private to each thread and then combined at the end of the loop. This is necessary because out is shared between threads and would otherwise be overwritten by each thread."
  },
  {
    "objectID": "slides/slides.html#benchmarking-2",
    "href": "slides/slides.html#benchmarking-2",
    "title": "Parallel computing with C++ from R",
    "section": "Benchmarking",
    "text": "Benchmarking\n\n\nCode\nx &lt;- rnorm(1e8)\nbm &lt;- bench::mark(\n  all(is.finite(x)),\n  is.finite(sum(x)),\n  all_finite_cpp(x),\n  all_finite_omp(x,  1L), \n  all_finite_omp(x,  2L),\n  all_finite_omp(x,  4L), \n  all_finite_omp(x,  8L),\n  iterations = bch_runs, \n  time_unit = \"ms\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgramming language\nExpression\nMedian computation time\nMemory allocation\n\n\n\n\nR\nall(is.finite(x))\n209.79 ms\n400 MB \n\n\nR\nis.finite(sum(x))\n154.48 ms\n   0 B  \n\n\nC++\nall_finite_cpp(x)\n 93.09 ms\n   0 B  \n\n\nC++ (OpenMP)\nall_finite_omp(x, 1L)\n 93.72 ms\n   0 B  \n\n\nC++ (OpenMP)\nall_finite_omp(x, 2L)\n 47.54 ms\n     4.7 kB\n\n\nC++ (OpenMP)\nall_finite_omp(x, 4L)\n 24.44 ms\n   0 B  \n\n\nC++ (OpenMP)\nall_finite_omp(x, 8L)\n 14.20 ms\n   0 B"
  },
  {
    "objectID": "slides/slides.html#cheatsheet-for-openmp",
    "href": "slides/slides.html#cheatsheet-for-openmp",
    "title": "Parallel computing with C++ from R",
    "section": "Cheatsheet for OpenMP",
    "text": "Cheatsheet for OpenMP"
  },
  {
    "objectID": "slides/slides.html#cheatsheet-for-openmp-1",
    "href": "slides/slides.html#cheatsheet-for-openmp-1",
    "title": "Parallel computing with C++ from R",
    "section": "Cheatsheet for OpenMP",
    "text": "Cheatsheet for OpenMP"
  },
  {
    "objectID": "slides/slides.html#openmp-in-an-r-package",
    "href": "slides/slides.html#openmp-in-an-r-package",
    "title": "Parallel computing with C++ from R",
    "section": "OpenMP in an R package",
    "text": "OpenMP in an R package\n\nSetup your package to use {Rcpp} (usethis::use_rcpp()).\nEdit src/Makevars file and add the following lines:\n\n\nPKG_CXXFLAGS = $(SHLIB_OPENMP_CXXFLAGS)\nPKG_LIBS = $(SHLIB_OPENMP_CXXFLAGS)\n\n\nCreate a C++ file src/omp_get_max_threads.cpp and start coding in it with OpenMP as we have seen (omitting the Rcpp::plugins() attributes)."
  },
  {
    "objectID": "slides/slides.html#thread-safety",
    "href": "slides/slides.html#thread-safety",
    "title": "Parallel computing with C++ from R",
    "section": "Thread safety",
    "text": "Thread safety\n\n\n\n\n\n\nDefinition\n\n\nA piece of code is thread-safe if it functions correctly during simultaneous execution by multiple threads. This is typically achieved by ensuring that shared data is accessed in a manner that avoids conflicts.\n\n\n\n\n\n\n\n\n\nR & Rcpp API’s are not thread-safe\n\n\nThe code that you write within parallel workers should not call the R or Rcpp API in any fashion. This is because R is single-threaded and concurrent interaction with its data structures can cause crashes and other undefined behavior.\n\n\nCalling any of the R API from threaded code is ‘for experts only’: they will need to read the source code to determine if it is thread-safe. In particular, code which makes use of the stack-checking mechanism must not be called from threaded code. Writing R Extensions (R Core Team, 2021)."
  },
  {
    "objectID": "slides/slides.html#two-consequences-for-r-users",
    "href": "slides/slides.html#two-consequences-for-r-users",
    "title": "Parallel computing with C++ from R",
    "section": "Two consequences for R users",
    "text": "Two consequences for R users\n\n\n\n\n\n\nProblem 1: Random number generation\n\n\nR’s C API provides access to the r*() functions for random number generation:\n\n#include &lt;Rcpp.h&gt;\n\n// [[Rcpp::export]]\ndouble rnorm_cpp()\n{\n    return R::rnorm(0, 1);\n}\n\nThey are not thread-safe and should not be called from within parallel workers.\n\n\n\n\n\n\n\n\n\nSolution to Problem 1\n\n\nUse a thread-safe generator such as the one provided by the {sitmo} package."
  },
  {
    "objectID": "slides/slides.html#two-big-consequences-for-r-users",
    "href": "slides/slides.html#two-big-consequences-for-r-users",
    "title": "Parallel computing with C++ from R",
    "section": "Two big consequences for R users",
    "text": "Two big consequences for R users\n\n\n\n\n\n\nProblem 2: Reading from and writing to R vectors and matrices\n\n\nNot being able to call the R or Rcpp API creates an obvious challenge: how to read and write to R vectors and matrices.\n\n\n\n\n\n\n\n\n\nSolution to Problem 2\n\n\nR vectors and matrices are just contiguous arrays of int, double, etc. Hence, they can be accessed using traditional array and pointer offsets. The {RcppParallel} package provides a convenient way to do this."
  },
  {
    "objectID": "slides/slides.html#sum-of-uniform-samples-r",
    "href": "slides/slides.html#sum-of-uniform-samples-r",
    "title": "Parallel computing with C++ from R",
    "section": "Sum of uniform samples (R)",
    "text": "Sum of uniform samples (R)\n\nsumunif &lt;- function(n, nstep, seed) {\n1  withr::with_seed(seed, {\n    rowSums(matrix(runif(n*nstep), n, nstep))   \n  })\n}\n\n\n1\n\nSet the seed for reproducibility. Using the {withr} package ensures that the seed is reset to its original value after the function call."
  },
  {
    "objectID": "slides/slides.html#sum-of-uniform-samples-c",
    "href": "slides/slides.html#sum-of-uniform-samples-c",
    "title": "Parallel computing with C++ from R",
    "section": "Sum of uniform samples (C++)",
    "text": "Sum of uniform samples (C++)\n\n//---------------------------------\n#include &lt;Rcpp.h&gt;\n1#include &lt;sitmo.h&gt;\n\n2// [[Rcpp::depends(sitmo)]]\n\n// [[Rcpp::export]]\nRcpp::NumericVector sumunif_sitmo(unsigned int n,\n                                  unsigned int nstep,\n                                  unsigned int seed)\n{\n  Rcpp::NumericVector out(n);\n3  sitmo::prng eng(seed);\n4  double mx = sitmo::prng::max();\n  double tmp = 0;\n\n  for (unsigned int i = 0;i &lt; n;++i)\n  {\n    tmp = 0.0;\n    for (unsigned int k = 0;k &lt; nstep;++k)\n5      tmp += eng() / mx;\n\n    out[i] = tmp;\n  }\n\n  return out;\n}\n\n\n1\n\nInclude the {sitmo} package header to access the prng class.\n\n2\n\nDeclare the dependency on the {sitmo} package so that the proper include and link flags are set at compile time. In a package, this would be done in the DESCRIPTION file by adding LinkingTo: sitmo.\n\n3\n\nCreate a prng object with the specified seed.\n\n4\n\nGet the maximum value that the generator can produce.\n\n5\n\nGenerate a uniform random number between 0 and 1."
  },
  {
    "objectID": "slides/slides.html#sum-of-uniform-samples-openmp",
    "href": "slides/slides.html#sum-of-uniform-samples-openmp",
    "title": "Parallel computing with C++ from R",
    "section": "Sum of uniform samples (OpenMP)",
    "text": "Sum of uniform samples (OpenMP)\n\n//---------------------------------\n#include &lt;Rcpp.h&gt;\n#include &lt;sitmo.h&gt;\n\n// // [[Rcpp::plugins(openmp)]] // Uncomment on Windows and Linux\n\n#ifdef _OPENMP\n1#include &lt;omp.h&gt;\n#endif\n\n// [[Rcpp::depends(sitmo)]]\n\n// [[Rcpp::export]]\nRcpp::NumericVector sumunif_sitmo_omp(unsigned int n,\n                                      unsigned int nstep,\n2                                      Rcpp::IntegerVector seeds)\n{\n  Rcpp::NumericVector out(n);\n\n3  unsigned int ncores = seeds.size();\n\n#ifdef _OPENMP\n4#pragma omp parallel num_threads(ncores)\n{\n#endif\n5  unsigned int seed = seeds[0];\n\n#ifdef _OPENMP\n6  seed = seeds[omp_get_thread_num()];\n#endif\n\n  sitmo::prng eng(seed);\n  double mx = sitmo::prng::max();\n  double tmp = 0;\n\n#ifdef _OPENMP\n7#pragma omp for\n#endif\n  for (unsigned int i = 0;i &lt; n;++i)\n  {\n    tmp = 0.0;\n    for (unsigned int k = 0;k &lt; nstep;++k)\n      tmp += eng() / mx;\n\n    out[i] = tmp;\n  }\n\n#ifdef _OPENMP\n8}\n#endif\n\nreturn out;\n}\n\n\n1\n\nInclude the OpenMP header file to access the omp_get_thread_num() function.\n\n2\n\nPass a vector of seeds to the function: this allows each thread to have its own seed.\n\n3\n\nGet the number of cores from the length of the seeds vector.\n\n4\n\nDefine the code section that will be parallelized and specify the number of threads.\n\n5\n\nSet the seed for the first thread to handle the case where OpenMP is not enabled.\n\n6\n\nSet the seed for each thread using each thead’s ID obtained from omp_get_thread_num().\n\n7\n\nParallelize the outer loop using the #pragma omp for directive.\n\n8\n\nClose the parallel region."
  },
  {
    "objectID": "slides/slides.html#benchmarking-3",
    "href": "slides/slides.html#benchmarking-3",
    "title": "Parallel computing with C++ from R",
    "section": "Benchmarking",
    "text": "Benchmarking\n\n\nCode\nn &lt;- 1e6\nnstep &lt;- 1e3\nseeds &lt;- sample.int(1e6, 8)\n\nbm &lt;- bench::mark(\n  rowSums(matrix(runif(n*nstep), n, nstep)),\n  sumunif_sitmo(n, nstep, seeds[1]),\n  sumunif_sitmo_omp(n, nstep, seeds[1:1]),\n  sumunif_sitmo_omp(n, nstep, seeds[1:2]),\n  sumunif_sitmo_omp(n, nstep, seeds[1:4]),\n  sumunif_sitmo_omp(n, nstep, seeds[1:8]),\n  iterations = bch_runs, \n  time_unit = \"s\", \n  check = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgramming language\nExpression\nMedian computation time\n\n\n\n\nR\nrowSums(matrix(runif(n * nstep), n, nstep))\n7.22 s\n\n\nC++\nsumunif_sitmo(n, nstep, seeds[1])\n2.36 s\n\n\nC++ (OpenMP)\nsumunif_sitmo_omp(n, nstep, seeds[1:1])\n2.43 s\n\n\nC++ (OpenMP)\nsumunif_sitmo_omp(n, nstep, seeds[1:2])\n1.22 s\n\n\nC++ (OpenMP)\nsumunif_sitmo_omp(n, nstep, seeds[1:4])\n0.62 s\n\n\nC++ (OpenMP)\nsumunif_sitmo_omp(n, nstep, seeds[1:8])\n0.33 s"
  },
  {
    "objectID": "slides/slides.html#overview-1",
    "href": "slides/slides.html#overview-1",
    "title": "Parallel computing with C++ from R",
    "section": "Overview",
    "text": "Overview\n{RcppParallel} provides a complete toolkit for creating portable, high-performance parallel algorithms without requiring direct manipulation of operating system threads.\n\n\n\n\n\n\nFeatures\n\n\n\nIntel TBB, a C++ library for task parallelism with a wide variety of parallel algorithms and data structures (Windows, OS X, Linux, and Solaris x86 only).\nTinyThread, a C++ library for portable use of operating system threads.\nRVector and RMatrix wrapper classes for safe and convenient access to R data structures in a multi-threaded environment.\nHigh level parallel functions (parallelFor() and parallelReduce()) that use Intel TBB as a back-end if supported and TinyThread otherwise."
  },
  {
    "objectID": "slides/slides.html#vector-and-matrix-accessor-classes",
    "href": "slides/slides.html#vector-and-matrix-accessor-classes",
    "title": "Parallel computing with C++ from R",
    "section": "Vector and matrix accessor classes",
    "text": "Vector and matrix accessor classes\n\n//---------------------------------\n#include &lt;Rcpp.h&gt;\n\n1// [[Rcpp::depends(RcppParallel)]]\n2#include &lt;RcppParallel.h&gt;\n\n// [[Rcpp::export]]\nRcpp::IntegerVector transformVector(Rcpp::IntegerVector x) {\n3  RcppParallel::RVector&lt;int&gt; input(x);\n4  Rcpp::IntegerVector y(x.size());\n5  RcppParallel::RVector&lt;int&gt; output(y);\n\n6\n\n7  return y;\n}\n\n\n1\n\nThe Rcpp attribute [[Rcpp::depends(RcppParallel)]] is used to indicate that the code depends on the RcppParallel package. It will ensure that the compilation flags are set correctly to compile the code.\n\n2\n\nInclude the RcppParallel.h header file to use the RVector and RMatrix classes.\n\n3\n\nCreate a threadsafe wrapper to the input Rcpp vector.\n\n4\n\nAllocate memory for the output vector.\n\n5\n\nCreate a threadsafe wrapper to the output vector.\n\n6\n\nPerform the desired transformation, possibly in parallel, using inputs stored in the input wrapper input and writing the results to the output wrapper output.\n\n7\n\nReturn the output Rcpp vector."
  },
  {
    "objectID": "slides/slides.html#rcppparallel-in-an-r-package",
    "href": "slides/slides.html#rcppparallel-in-an-r-package",
    "title": "Parallel computing with C++ from R",
    "section": "{RcppParallel} in an R package",
    "text": "{RcppParallel} in an R package\n\n\n\n\n\nDESCRIPTION\n\n\nImports: RcppParallel\nLinkingTo: Rcpp, RcppParallel\nSystemRequirements: GNU make\n\n\n\n\n\n\nNAMESPACE\n\n\nimportFrom(RcppParallel, RcppParallelLibs)\n\nIf you are using {roxygen2} to generate the NAMESPACE file, you can add the following line to the packagename-package.R file:\n\n\n#' @importFrom RcppParallel RcppParallelLibs\n\n\nwhich will automatically populate the NAMESPACE upon devtools::document().\n\n\n\n\n\n\n\n\nsrc/Makevars\n\n\nPKG_LIBS += $(shell ${R_HOME}/bin/Rscript -e \"RcppParallel::RcppParallelLibs()\")\n\n\n\n\n\n\nsrc/Makevars.win\n\n\nPKG_CXXFLAGS += -DRCPP_PARALLEL_USE_TBB=1\nPKG_LIBS += $(shell \"${R_HOME}/bin${R_ARCH_BIN}/Rscript.exe\" \\\n          -e \"RcppParallel::RcppParallelLibs()\")\n\n\n\n\n\n\n\nWorkflow\n\n\nNow simply include the main RcppParallel.h header file in source files that need to use it:\n\n#include &lt;RcppParallel.h&gt;"
  },
  {
    "objectID": "slides/slides.html#error-function-c",
    "href": "slides/slides.html#error-function-c",
    "title": "Parallel computing with C++ from R",
    "section": "Error function (C++)",
    "text": "Error function (C++)\nWe will illustrate the use of {RcppParallel} by implementing the error function in C++.\n\n//---------------------------------\n#include &lt;Rcpp.h&gt;\n\n1// [[Rcpp::depends(BH)]]\n2#include &lt;boost/math/special_functions/erf.hpp&gt;\n\n// [[Rcpp::export]]\nRcpp::NumericVector erf_cpp(Rcpp::NumericVector x) {\n  Rcpp::NumericVector y(x.size());\n\n  for (int i = 0; i &lt; x.size(); i++) {\n3    y[i] = boost::math::erf(x[i]);\n  }\n\n  return y;\n}\n\n\n1\n\nThe Rcpp attribute [[Rcpp::depends(BH)]] is used to indicate that the code depends on the Boost C++ libraries. It will ensure that the compilation flags are set correctly to compile the code.\n\n2\n\nInclude the boost/math/special_functions/erf.hpp header file to use the boost::math::erf() function.\n\n3\n\nCompute the error function for each element of the input vector x and store the results in the output vector y."
  },
  {
    "objectID": "slides/slides.html#error-function-r",
    "href": "slides/slides.html#error-function-r",
    "title": "Parallel computing with C++ from R",
    "section": "Error function (R)",
    "text": "Error function (R)\nNow, let us define the error function in R for comparison:\n\nerf_r &lt;- function(x) {\n  2 * pnorm(x * sqrt(2)) - 1\n}\n\nLet us check that both R and C++ versions of erf() provide the same results:\n\nx &lt;- rnorm(1e6)\nmax(abs(erf_r(x) - erf_cpp(x)))\n\n[1] 4.440892e-16\n\n\nThe numerical difference is of the order of the machine precision."
  },
  {
    "objectID": "slides/slides.html#error-function-rcppparallel-openmp",
    "href": "slides/slides.html#error-function-rcppparallel-openmp",
    "title": "Parallel computing with C++ from R",
    "section": "Error function (RcppParallel + OpenMP)",
    "text": "Error function (RcppParallel + OpenMP)\n\n//---------------------------------\n#include &lt;Rcpp.h&gt;\n\n// [[Rcpp::depends(BH)]]\n#include &lt;boost/math/special_functions/erf.hpp&gt;\n\n// [[Rcpp::depends(RcppParallel)]]\n#include &lt;RcppParallel.h&gt;\n\n// // [[Rcpp::plugins(openmp)]] // Remove `//` on Windows and Linux\n\n// [[Rcpp::export]]\nRcpp::NumericVector erf_omp(Rcpp::NumericVector x, unsigned int ncores)\n{\n  unsigned int n = x.size();\n  Rcpp::NumericVector out(n);\n\n1  RcppParallel::RVector&lt;double&gt; wo(out);\n2  RcppParallel::RVector&lt;double&gt; wx(x);\n\n#ifdef _OPENMP\n3#pragma omp parallel for num_threads(ncores)\n#endif\n  for (unsigned int i = 0;i &lt; n;++i)\n4    wo[i] = boost::math::erf(wx[i]);\n\n  return out;\n}\n\n\n1\n\nUse the threadsafe wrapper class RVector of {RcppParallel} to manipulate the output vector.\n\n2\n\nUse the threadsafe wrapper class RVector of {RcppParallel} to manipulate the input vector.\n\n3\n\nInsert the appropriate OpenMP clauses and directives.\n\n4\n\nUse the wrapped objects to perform the computation within the workers."
  },
  {
    "objectID": "slides/slides.html#error-function-rcppparallel",
    "href": "slides/slides.html#error-function-rcppparallel",
    "title": "Parallel computing with C++ from R",
    "section": "Error function (RcppParallel)",
    "text": "Error function (RcppParallel)\n\n//---------------------------------\n#include &lt;Rcpp.h&gt;\n\n// [[Rcpp::depends(BH)]]\n#include &lt;boost/math/special_functions/erf.hpp&gt;\n\n// [[Rcpp::depends(RcppParallel)]]\n#include &lt;RcppParallel.h&gt;\n\n1struct ErfFunctor : public RcppParallel::Worker {\n  // Threadsafe wrapper around input vector\n2  const RcppParallel::RVector&lt;double&gt; m_InputVector;\n\n  // Threadsafe wrapper around output vector\n3  RcppParallel::RVector&lt;double&gt; m_OutputVector;\n\n  // initialize with input and output vectors\n  ErfFunctor(const Rcpp::NumericVector input, Rcpp::NumericVector output)\n4    : m_InputVector(input), m_OutputVector(output) {}\n\n  // function call operator that work for the specified range (begin/end)\n5  void operator()(std::size_t begin, std::size_t end) {\n    for (unsigned int i = begin;i &lt; end;++i) {\n      m_OutputVector[i] = boost::math::erf(m_InputVector[i]);\n    }\n  }\n};\n\n// [[Rcpp::export]]\n6Rcpp::NumericVector erf_parallel_impl(Rcpp::NumericVector x) {\n  Rcpp::NumericVector y(x.size());\n\n  ErfFunctor erfFunctor(x, y);\n  RcppParallel::parallelFor(0, x.size(), erfFunctor);\n\n  return y;\n}\n\n\n1\n\nDefine a functor class ErfFunctor that inherits from RcppParallel::Worker for later use with RcppParallel::parallelFor().\n\n2\n\nDefine a first attribute m_InputVector for the functor class that is a threadsafe wrapper around an input vector.\n\n3\n\nDefine a second attribute m_OutputVector for the functor class that is a threadsafe wrapper around an output vector.\n\n4\n\nDefine a constructor for the functor class that takes an input and output vectors and initializes the two corresponding class attributes.\n\n5\n\nDefine the function call operator operator() that will be called by RcppParallel::parallelFor() for the specific range that the worker will have to process.\n\n6\n\nDefine the main function erf_parallel_impl() that will be exported to R and will be used to call the parallel computation. This function will create an instance of the functor class and call RcppParallel::parallelFor() to perform the parallel computation."
  },
  {
    "objectID": "slides/slides.html#error-function---control-ncores",
    "href": "slides/slides.html#error-function---control-ncores",
    "title": "Parallel computing with C++ from R",
    "section": "Error function - Control ncores",
    "text": "Error function - Control ncores\nThe previously defined function erf_parallel_impl() has no way to control the number of cores used for the computation. We can define a wrapper function that will set the number of cores before calling erf_parallel_impl() and reset it afterwards:\n\nerf_parallel &lt;- function(x, ncores) {\n  on.exit(RcppParallel::setThreadOptions())\n  RcppParallel::setThreadOptions(numThreads = ncores)\n  erf_parallel_impl(x)\n}"
  },
  {
    "objectID": "slides/slides.html#benchmarking-4",
    "href": "slides/slides.html#benchmarking-4",
    "title": "Parallel computing with C++ from R",
    "section": "Benchmarking",
    "text": "Benchmarking\n\n\nCode\nbm &lt;- bench::mark(\n  erf_r(x),\n  erf_cpp(x),\n  erf_omp(x, 1),\n  erf_omp(x, 2),\n  erf_omp(x, 4),\n  erf_omp(x, 8),\n  erf_parallel(x, 1),\n  erf_parallel(x, 2),\n  erf_parallel(x, 4),\n  erf_parallel(x, 8),\n  iterations = bch_runs, \n  time_unit = \"ms\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgramming language\nExpression\nMedian computation time\nMemory allocation\n\n\n\n\nR\nerf_r(x)\n27.07 ms\n16 MB \n\n\nC++\nerf_cpp(x)\n18.40 ms\n 8 MB \n\n\nC++ (OpenMP)\nerf_omp(x, 1)\n17.97 ms\n 8 MB \n\n\nC++ (OpenMP)\nerf_omp(x, 2)\n 9.47 ms\n 8 MB \n\n\nC++ (OpenMP)\nerf_omp(x, 4)\n 5.33 ms\n 8 MB \n\n\nC++ (OpenMP)\nerf_omp(x, 8)\n 3.01 ms\n 8 MB \n\n\nC++ (RcppParallel)\nerf_parallel(x, 1)\n19.33 ms\n 8 MB \n\n\nC++ (RcppParallel)\nerf_parallel(x, 2)\n10.17 ms\n 8 MB \n\n\nC++ (RcppParallel)\nerf_parallel(x, 4)\n 5.46 ms\n 8 MB \n\n\nC++ (RcppParallel)\nerf_parallel(x, 8)\n 3.19 ms\n 8 MB"
  },
  {
    "objectID": "slides/slides.html#progress-bars-via-rcppprogress",
    "href": "slides/slides.html#progress-bars-via-rcppprogress",
    "title": "Parallel computing with C++ from R",
    "section": "Progress bars via {RcppProgress}",
    "text": "Progress bars via {RcppProgress}\n\n\n\n\n\n\n{RcppProgress}\n\n\nThe {RcppProgress} package provides a way to display progress bars in Rcpp code. This is useful when you have long-running C++ code and want to provide feedback to the user. It is also compatible with OpenMP parallel code.\n\n\n\n\n\n\n\n\n\nCompatibility with {RcppParallel}\n\n\nProgress bars generated by the {RcppProgress} package are not readily compatible with the {RcppParallel} framework. See this issue."
  },
  {
    "objectID": "slides/slides.html#progress-bars-with-openmp",
    "href": "slides/slides.html#progress-bars-with-openmp",
    "title": "Parallel computing with C++ from R",
    "section": "Progress bars with OpenMP",
    "text": "Progress bars with OpenMP\n\n//---------------------------------\n#include &lt;Rcpp.h&gt;\n\n// // [[Rcpp::plugins(openmp)]] // Uncomment on Windows and Linux\n\n// [[Rcpp::depends(BH)]]\n#include &lt;boost/math/special_functions/erf.hpp&gt;\n\n// [[Rcpp::depends(RcppParallel)]]\n#include &lt;RcppParallel.h&gt;\n\n1// [[Rcpp::depends(RcppProgress)]]\n2#include &lt;progress.hpp&gt;\n#include &lt;progress_bar.hpp&gt;\n\n// [[Rcpp::export]]\nRcpp::NumericVector erf_omp_progress(Rcpp::NumericVector x,\n                                     unsigned int ncores,\n3                                     bool display_progress = false)\n{\n  unsigned int n = x.size();\n  Rcpp::NumericVector out(n);\n4  Progress p(n, display_progress);\n\n  RcppParallel::RVector&lt;double&gt; wo(out);\n  RcppParallel::RVector&lt;double&gt; wx(x);\n\n#ifdef _OPENMP\n#pragma omp parallel for num_threads(ncores)\n#endif\n  for (unsigned int i = 0;i &lt; n;++i)\n  {\n5    if (!Progress::check_abort())\n    {\n6      p.increment();\n      wo[i] = boost::math::erf(wx[i]);\n    }\n  }\n\n  return out;\n}\n\n\n1\n\nThe Rcpp attribute Rcpp::depends(RcppProgress) ensures that compilation flags to link to the {RcppProgress} headers are properly set.\n\n2\n\nInclude the necessary headers to provide access to the Progress class.\n\n3\n\nAdd a flag to turn on progress bar display as optional argument to your function which defaults to false.\n\n4\n\nInstantiate a progress bar via the Progress class which takes as arguments the number of total number of increments the bar should achieve and whether the progress bar should be displayed.\n\n5\n\nWithin workers, check that user did not abort the calculation.\n\n6\n\nWithin workers, increment the progress bar.\n\n\n\n\n\nx &lt;- rnorm(1e7)\ny &lt;- erf_omp_progress(x, 4, display_progress = TRUE)"
  },
  {
    "objectID": "slides/slides.html#thread-safe-communication-with-r",
    "href": "slides/slides.html#thread-safe-communication-with-r",
    "title": "Parallel computing with C++ from R",
    "section": "Thread safe communication with R",
    "text": "Thread safe communication with R\n\n\n\n\n\n\nPrinting messages to the console\n\n\nProblem. You might want to print out messages to the R console sometimes. {Rcpp} provides the Rcpp::Rcout replacement of std::cout which correctly places the messages in the R console. It is however not threadsafe.\nSolution. {RcppThread} provides RcppThread::Rcout and RcppThread::Rcerr which are treadsafe.\n\n\n\n\n\n\n\n\n\nInterrupting computations\n\n\nProblem. It is good practice in long-running computations to allow the user to interrupt manually the computation. This needs to be handled on the developer side. {Rcpp} provides the Rcpp::checkUserInterrupt() function for this purpose but, as the rest of the API, it is not threadsafe.\nSolution. {RcppThread} provides RcppThread::checkUserInterrupt which is treadsafe."
  },
  {
    "objectID": "slides/slides.html#multithreading-with-stdthread",
    "href": "slides/slides.html#multithreading-with-stdthread",
    "title": "Parallel computing with C++ from R",
    "section": "Multithreading with std::thread",
    "text": "Multithreading with std::thread\n\n\n\n\n\n\nThe RcppThread::Thread class\n\n\n{RcppThread}’s Thread class is an R-friendly wrapper to std::thread. Instances of class Thread behave almost like instances of std::thread. There is one important difference: Whenever child threads are running, the main thread periodically synchronizes with R. In particular, it checks for user interruptions and releases all messages passed to RcppThread::Rcout and RcppThread::Rcerr. When the user interrupts a threaded computation, any thread will stop as soon it encounters RcppThread::checkUserInterrupt()."
  },
  {
    "objectID": "slides/slides.html#multithreading-with-stdthread-1",
    "href": "slides/slides.html#multithreading-with-stdthread-1",
    "title": "Parallel computing with C++ from R",
    "section": "Multithreading with std::thread",
    "text": "Multithreading with std::thread\n\n#include &lt;Rcpp.h&gt;\n\n1// [[Rcpp::plugins(cpp11)]]\n2// [[Rcpp::depends(RcppThread)]]\n3#include &lt;RcppThread.h&gt;\n\n// [[Rcpp::export]]\nvoid pyjamaParty()\n{\n4  auto job = [] (int id) {\n    std::this_thread::sleep_for(std::chrono::seconds(1));\n    RcppThread::Rcout &lt;&lt; id &lt;&lt; \" slept for one second\" &lt;&lt; std::endl;\n    RcppThread::checkUserInterrupt();\n    std::this_thread::sleep_for(std::chrono::seconds(1));\n    RcppThread::Rcout &lt;&lt; id &lt;&lt; \" slept for another second\" &lt;&lt; std::endl;\n  };\n5  RcppThread::Thread t1(job, 1);\n  RcppThread::Thread t2(job, 2);\n6  t1.join();\n  t2.join();\n}\n\n\n1\n\nRcpp attribute to enable C++11 features.\n\n2\n\nRcpp attribute to ensure that the package is linked to the {RcppThread} headers with the proper compilation flags.\n\n3\n\nInclude the necessary header to provide access to the Thread class, RcppThread::Rcout, and RcppThread::checkUserInterrupt(). It also includes the standard library headers required for std::thread and std::chrono.\n\n4\n\nDefine the task to be executed by the threads as a lambda function job which takes an integer id as argument and does the following: Sleep for one second, send a message, check for a user interruption, go back to sleep, and send another message.\n\n5\n\nSpawn two new Thread’s with this job and different id’s. Notice that the argument of the job function is passed to the ‘Thread’ constructor. More generally, if a job function takes arguments, they must be passed to the constructor as a comma-separated list.\n\n6\n\nThreads should always be joined before they are destructed. The .join() statements signal the main thread to wait until the jobs have finished. But instead of just waiting, the main thread starts synchronizing with R and checking for user interruptions."
  },
  {
    "objectID": "slides/slides.html#thread-pool---basic-usage",
    "href": "slides/slides.html#thread-pool---basic-usage",
    "title": "Parallel computing with C++ from R",
    "section": "Thread pool - Basic usage",
    "text": "Thread pool - Basic usage\n\n#include &lt;Rcpp.h&gt;\n\n// [[Rcpp::plugins(cpp11)]]\n// [[Rcpp::depends(RcppThread)]]\n#include &lt;RcppThread.h&gt;\n\n// [[Rcpp::export]]\nstd::vector&lt;unsigned int&gt; rcpp_thread_example1(unsigned int n,\n                                               unsigned int ncores)\n{\n1  RcppThread::ThreadPool pool(ncores);\n  std::vector&lt;unsigned int&gt; x(n);\n2  auto task = [&x] (unsigned int i) { x[i] = i; };\n  for (unsigned int i = 0;i &lt; x.size();++i)\n3    pool.push(task, i);\n4  pool.join();\n  return x;\n}\n\n\n1\n\nCreate a thread pool with ncores threads. Useful when the number of tasks is known in advance.\n\n2\n\nDefine the task to be executed by the threads as a lambda function task which takes an integer i as argument and assigns i to the i-th element of x. The lambda function captures x by reference. This is necessary because the lambda function is executed in a different context than the main thread.\n\n3\n\nPush the task to the thread pool for each element of x. The push method takes the task and its arguments as arguments.\n\n4\n\nWait for all threads to finish. This is necessary because the main thread should not finish before the threads have finished. It also starts synchronizing with R and checking for user interruptions.\n\n\n\n\n\nrcpp_thread_example1(10, 3)\n\n [1] 0 1 2 3 4 5 6 7 8 9"
  },
  {
    "objectID": "slides/slides.html#thread-pool-which-returns-a-value",
    "href": "slides/slides.html#thread-pool-which-returns-a-value",
    "title": "Parallel computing with C++ from R",
    "section": "Thread pool which returns a value",
    "text": "Thread pool which returns a value\n\n#include &lt;Rcpp.h&gt;\n\n// [[Rcpp::plugins(cpp11)]]\n// [[Rcpp::depends(RcppThread)]]\n#include &lt;RcppThread.h&gt;\n\n// [[Rcpp::export]]\nstd::vector&lt;unsigned int&gt; rcpp_thread_example2(unsigned int n,\n                                               unsigned int ncores)\n{\n  RcppThread::ThreadPool pool(ncores);\n\n1  std::vector&lt;unsigned int&gt; x(n);\n  for (unsigned int i = 0;i &lt; n;++i)\n    x[i] = i + 1;\n\n2  auto task = [&x] (unsigned int i) {\n    return x[i] * x[i];\n  };\n\n3  std::vector&lt;std::future&lt;unsigned int&gt;&gt; futures(n);\n  for (unsigned int i = 0;i &lt; n;++i)\n    futures[i] = pool.pushReturn(task, i);\n\n4  std::vector&lt;unsigned int&gt; results(n);\n  for (unsigned int i = 0;i &lt; n;++i)\n    results[i] = futures[i].get();\n\n5  pool.join();\n\n  return results;\n}\n\n\n1\n\nCreate a vector x of n elements and initialize it with the integers from 1 to n.\n\n2\n\nDefine the task to be executed by the threads as a lambda function task which takes an integer i as argument and returns the square of the i-th element of x. The lambda function captures x by reference. This is necessary because the lambda function is executed in a different context than the main thread.\n\n3\n\nCreate a vector of std::future objects to store the results of the tasks because the pushReturn() method returns a std::future object which can be used to later retrieve the result of the task.\n\n4\n\nRetrieve the results of the tasks from the std::future objects and store them in a vector results.\n\n5\n\nWait for all threads to finish. This is necessary because the main thread should not finish before the threads have finished. It also starts synchronizing with R and checking for user interruptions.\n\n\n\n\n\nrcpp_thread_example2(10, 3)\n\n [1]   1   4   9  16  25  36  49  64  81 100"
  },
  {
    "objectID": "slides/slides.html#parallel-for-loop",
    "href": "slides/slides.html#parallel-for-loop",
    "title": "Parallel computing with C++ from R",
    "section": "Parallel for loop",
    "text": "Parallel for loop\n\n#include &lt;Rcpp.h&gt;\n\n// [[Rcpp::plugins(cpp11)]]\n// [[Rcpp::depends(RcppThread)]]\n#include &lt;RcppThread.h&gt;\n\n// [[Rcpp::export]]\nstd::vector&lt;unsigned int&gt; parallelfor_example(unsigned int n)\n{\n  // Index-based\n  std::vector&lt;unsigned int&gt; x(n);\n\n  auto task = [&x] (unsigned int i) {\n    x[i] = i;\n  };\n\n1  RcppThread::parallelFor(0, x.size(), task);\n\n  return x;\n}\n\n\n1\n\nExecute the task for each element of x in parallel. The parallelFor() function takes the start index, end index, and the task as arguments.\n\n\n\n\n\nparallelfor_example(10)\n\n [1] 0 1 2 3 4 5 6 7 8 9"
  },
  {
    "objectID": "slides/slides.html#parallel-for-each-loop",
    "href": "slides/slides.html#parallel-for-each-loop",
    "title": "Parallel computing with C++ from R",
    "section": "Parallel for-each loop",
    "text": "Parallel for-each loop\n\n#include &lt;Rcpp.h&gt;\n\n// [[Rcpp::plugins(cpp11)]]\n// [[Rcpp::depends(RcppThread)]]\n#include &lt;RcppThread.h&gt;\n\n// [[Rcpp::export]]\nstd::vector&lt;unsigned int&gt; parallelforeach_example(unsigned int n)\n{\n  // Over elements of a vector\n  std::vector&lt;unsigned int&gt; x(n);\n  for (unsigned int i = 0;i &lt; n;++i)\n    x[i] = i;\n\n1  auto task = [] (unsigned int &xx) {\n    xx *= 2;\n  };\n\n2  RcppThread::parallelForEach(x, task);\n\n  return x;\n}\n\n\n1\n\nDefine the task to be executed by the threads as a lambda function task which takes an integer xx as argument and multiplies it by 2. The argument is passed by reference because the task modifies the argument.\n\n2\n\nExecute the task for each element of x in parallel. The parallelForEach() function takes the vector and the task as arguments and applies the task to each element of the vector.\n\n\n\n\n\nparallelforeach_example(10)\n\n [1]  0  2  4  6  8 10 12 14 16 18"
  },
  {
    "objectID": "slides/slides.html#rcppthread-in-an-r-package",
    "href": "slides/slides.html#rcppthread-in-an-r-package",
    "title": "Parallel computing with C++ from R",
    "section": "{RcppThread} in an R package",
    "text": "{RcppThread} in an R package\nUsing {RcppThread} in an R package is easy:\n\nAdd CXX_STD = CXX11 to the src/Makevars(.win) files of your package.\nAdd RcppThread to the LinkingTo field in the DESCRIPTION file.\nInclude the headers with #include \"RcppThread.h\" in your C++ source files within the src/ directory."
  },
  {
    "objectID": "slides/slides.html#progress-report-1",
    "href": "slides/slides.html#progress-report-1",
    "title": "Parallel computing with C++ from R",
    "section": "Progress report",
    "text": "Progress report\n\n#include &lt;Rcpp.h&gt;\n\n// [[Rcpp::plugins(cpp11)]]\n// [[Rcpp::depends(RcppThread)]]\n#include &lt;RcppThread.h&gt;\n\n// [[Rcpp::export]]\nvoid pb_example()\n{\n  // 20 iterations in loop, update progress every 1 sec\n1  RcppThread::ProgressBar bar(20, 1);\n2  RcppThread::parallelFor(0, 20, [&] (int i) {\n    std::this_thread::sleep_for(std::chrono::milliseconds(200));\n    ++bar;\n  });\n}\n\n\n1\n\nCreate a progress bar with 20 iterations and update the progress every 1 second.\n\n2\n\nExecute the task for each iteration in parallel. The task instructs the thread to sleep for 200 milliseconds and then increment the progress bar.\n\n\n\n\n\npb_example()\n\n\nComputing: [======================                  ] 55%  (~0s remaining)       \nComputing: [========================================] 100% (done)"
  },
  {
    "objectID": "slides/slides.html#references",
    "href": "slides/slides.html#references",
    "title": "Parallel computing with C++ from R",
    "section": "References",
    "text": "References\n\n\n\n\nHigh-Performance Computing with R - Fréjus - aymeric.stamm@cnrs.fr - https://astamm.github.io/parallel-rcpp/\n\n\n\n\n“Beginning OpenMP.” http://chryswoods.com/beginning_openmp/.\n\n\nDagum, Leonardo, and Ramesh Menon. 1998. “OpenMP: An Industry Standard API for Shared-Memory Programming.” IEEE Computational Science and Engineering 5 (1): 46–55.\n\n\nNagler, Thomas. 2021. “R-Friendly Multi-Threading in c++.” Journal of Statistical Software, Code Snippets 97 (1): 1–18. https://doi.org/10.18637/jss.v097.c01.\n\n\n“OpenMP API Reference Guide.” https://www.openmp.org/wp-content/uploads/OpenMPRefGuide-5.2-Web-2024.pdf.\n\n\n“OpenMP Reference Sheet for C/C++.” n.d. https://cheat-sheets.org/saved-copy/OpenMP_reference.pdf.\n\n\nReinders, James. 2007. Intel Threading Building Blocks, Outfitting c++ for Multi-Core Processor Parallelism. O’Reilly Media, Inc.\n\n\nSchäling, Boris. 2014. The Boost c++ Libraries. Vol. 3. XML press Laguna Hills."
  },
  {
    "objectID": "lab/hausdorff_dist_solutions.html",
    "href": "lab/hausdorff_dist_solutions.html",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "",
    "text": "We have simulated 3D functional data for this lab that is provided in the Quarto document in the dat object.\nThe dat object is a list of size \\(100\\) containing \\(100\\) three-dimensional curves observed on a common grid of size \\(200\\) of the interval \\([0, 1]\\).\nAs a result, each element of the dat list is a \\(3 \\times 200\\) matrix.\nHere we focus on a subset of the data, the first \\(21\\) curves, which looks like:\n\nplot(mfdat)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObjective\n\n\n\nThe goal is to implement a function similar to stats::dist() which computes the pairwise distance matrix on this functional dataset using the Hausdorff distance."
  },
  {
    "objectID": "lab/hausdorff_dist_solutions.html#data-goal",
    "href": "lab/hausdorff_dist_solutions.html#data-goal",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "",
    "text": "We have simulated 3D functional data for this lab that is provided in the Quarto document in the dat object.\nThe dat object is a list of size \\(100\\) containing \\(100\\) three-dimensional curves observed on a common grid of size \\(200\\) of the interval \\([0, 1]\\).\nAs a result, each element of the dat list is a \\(3 \\times 200\\) matrix.\nHere we focus on a subset of the data, the first \\(21\\) curves, which looks like:\n\nplot(mfdat)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObjective\n\n\n\nThe goal is to implement a function similar to stats::dist() which computes the pairwise distance matrix on this functional dataset using the Hausdorff distance."
  },
  {
    "objectID": "lab/hausdorff_dist_solutions.html#hausdorff-distance-in-r",
    "href": "lab/hausdorff_dist_solutions.html#hausdorff-distance-in-r",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "Hausdorff distance in R",
    "text": "Hausdorff distance in R\nWe can implement the Hausdorff distance between two curves as:\n\nhausdorff_distance_vec &lt;- function(x, y) {\n  P &lt;- ncol(x)\n  dX &lt;- 1:P |&gt;\n    purrr::map_dbl(\\(p) {\n      min(colSums((y - x[, p])^2))\n    }) |&gt;\n    max()\n  dY &lt;- 1:P |&gt;\n    purrr::map_dbl(\\(p) {\n      min(colSums((x - y[, p])^2))\n    }) |&gt;\n    max()\n  sqrt(max(dX, dY))\n}\n\nThis version exploits the vectorized nature of R to compute the Hausdorff distance via calls to colSums() and max(). Another version based on a double loop is provided by the following hausdorff_distance_for() function:\n\nhausdorff_distance_for &lt;- function(x, y) {\n  P &lt;- ncol(x)\n  dX &lt;- 0\n  dY &lt;- 0\n  for (i in 1:P) {\n    min_dist_x &lt;- Inf\n    min_dist_y &lt;- Inf\n    for (j in 1:P) {\n      dist_x &lt;- sum((y[, j] - x[, i])^2)\n      if (dist_x &lt; min_dist_x) {\n        min_dist_x &lt;- dist_x\n      }\n      dist_y &lt;- sum((x[, j] - y[, i])^2)\n      if (dist_y &lt; min_dist_y) {\n        min_dist_y &lt;- dist_y\n      }\n    }\n    if (min_dist_x &gt; dX) {\n      dX &lt;- min_dist_x\n    }\n    if (min_dist_y &gt; dY) {\n      dY &lt;- min_dist_y\n    }\n  }\n  sqrt(max(dX, dY))\n}\n\nWe can benchmark the two versions:\n\nbm &lt;- bench::mark(\n  hausdorff_distance_vec(dat[[1]], dat[[2]]),\n  hausdorff_distance_for(dat[[1]], dat[[2]])\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpression\nMedian computation time\nMemory allocation\n\n\n\n\nhausdorff_distance_vec(dat[[1]], dat[[2]])\n 2.02 ms\n  2.7 MB\n\n\nhausdorff_distance_for(dat[[1]], dat[[2]])\n50.08 ms\n124.6 kB\n\n\n\n\n\n\n\nWe conclude that the vectorized version is faster but has a huge memory footprint compared to the loop-based version. This means that the vectorized version is not suitable for even moderately large data sets."
  },
  {
    "objectID": "lab/hausdorff_dist_solutions.html#pairwise-distance-matrix-in-r",
    "href": "lab/hausdorff_dist_solutions.html#pairwise-distance-matrix-in-r",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "Pairwise distance matrix in R",
    "text": "Pairwise distance matrix in R\n\n\n\n\n\n\ndist objects\n\n\n\nTake a look at the documentation of the stats::dist() function to understand how to make an object of class dist.\n\n\nWe can exploit the previous functions to compute the pairwise distance matrix using the Hausdorff distance:\n\ndist_r_v1 &lt;- function(x, vectorized = FALSE) {\n  hausdorff_distance &lt;- if (vectorized) \n    hausdorff_distance_vec\n  else \n    hausdorff_distance_for\n  N &lt;- length(x)\n  out &lt;- 1:(N - 1) |&gt;\n    purrr::map(\\(i) {\n      purrr::map_dbl((i + 1):N, \\(j) {\n        hausdorff_distance(x[[i]], x[[j]])\n      })\n    }) |&gt;\n    purrr::list_c()\n\n  attributes(out) &lt;- NULL\n  attr(out, \"Size\") &lt;- N\n  lbls &lt;- names(x)\n  attr(out, \"Labels\") &lt;- if (is.null(lbls)) 1:N else lbls\n  attr(out, \"Diag\") &lt;- FALSE\n  attr(out, \"Upper\") &lt;- FALSE\n  attr(out, \"method\") &lt;- \"hausdorff\"\n  class(out) &lt;- \"dist\"\n  out\n}\n\nWe can benchmark the two versions:\n\nbm &lt;- bench::mark(\n  dist_r_v1(dat, vectorized = TRUE),\n  dist_r_v1(dat, vectorized = FALSE)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpression\nMedian computation time\nMemory allocation\n\n\n\n\ndist_r_v1(dat, vectorized = TRUE)\n 0.48 s\n 546.5 MB\n\n\ndist_r_v1(dat, vectorized = FALSE)\n11.18 s\n3 kB \n\n\n\n\n\n\n\n\n\n\n\n\n\nMemory footprint\n\n\n\nWe confirm that the vectorized version is not scalable to large datasets. Using it on the full dataset actually requires 12GB of memory! We will therefore focus on the loop-based version from now on."
  },
  {
    "objectID": "lab/hausdorff_dist_solutions.html#futureverse",
    "href": "lab/hausdorff_dist_solutions.html#futureverse",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "futureverse",
    "text": "futureverse\n\nParallelize outer loop\n\ndist_r_v2 &lt;- function(x) {\n  N &lt;- length(x)\n  out &lt;- 1:(N - 1) |&gt;\n    furrr::future_map(\\(i) {\n      purrr::map_dbl((i + 1):N, \\(j) {\n        hausdorff_distance_for(x[[i]], x[[j]])\n      })\n    }) |&gt;\n    purrr::list_c()\n\n  attributes(out) &lt;- NULL\n  attr(out, \"Size\") &lt;- N\n  lbls &lt;- names(x)\n  attr(out, \"Labels\") &lt;- if (is.null(lbls)) 1:N else lbls\n  attr(out, \"Diag\") &lt;- FALSE\n  attr(out, \"Upper\") &lt;- FALSE\n  attr(out, \"method\") &lt;- \"hausdorff\"\n  class(out) &lt;- \"dist\"\n  out\n}\n\n\n\nTweaking the chunk size\n\ndist_r_v3 &lt;- function(x) {\n  N &lt;- length(x)\n  out &lt;- 1:(N - 1) |&gt;\n    furrr::future_map(\\(i) {\n      purrr::map_dbl((i + 1):N, \\(j) {\n        hausdorff_distance_for(x[[i]], x[[j]])\n      })\n    }, .options = furrr::furrr_options(chunk_size = 1)) |&gt;\n    purrr::list_c()\n\n  attributes(out) &lt;- NULL\n  attr(out, \"Size\") &lt;- N\n  lbls &lt;- names(x)\n  attr(out, \"Labels\") &lt;- if (is.null(lbls)) 1:N else lbls\n  attr(out, \"Diag\") &lt;- FALSE\n  attr(out, \"Upper\") &lt;- FALSE\n  attr(out, \"method\") &lt;- \"hausdorff\"\n  class(out) &lt;- \"dist\"\n  out\n}\n\n\n\nNested plan\n\ndist_r_v4 &lt;- function(x) {\n  N &lt;- length(x)\n  out &lt;- 1:(N - 1) |&gt;\n    furrr::future_map(\\(i) {\n      furrr::future_map_dbl((i + 1):N, \\(j) {\n        hausdorff_distance_for(x[[i]], x[[j]])\n      })\n    }, .options = furrr::furrr_options(chunk_size = 1)) |&gt;\n    purrr::list_c()\n\n  attributes(out) &lt;- NULL\n  attr(out, \"Size\") &lt;- N\n  lbls &lt;- names(x)\n  attr(out, \"Labels\") &lt;- if (is.null(lbls)) 1:N else lbls\n  attr(out, \"Diag\") &lt;- FALSE\n  attr(out, \"Upper\") &lt;- FALSE\n  attr(out, \"method\") &lt;- \"hausdorff\"\n  class(out) &lt;- \"dist\"\n  out\n}\n\n\n\nConvert to single loop\n\ndist_r_v5 &lt;- function(x) {\n  N &lt;- length(x)\n  K &lt;- N * (N - 1) / 2\n  out &lt;- furrr::future_map_dbl(1:K, \\(k) {\n    k &lt;- k - 1\n    i &lt;- N - 2 - floor(sqrt(-8 * k + 4 * N * (N - 1) - 7) / 2.0 - 0.5);\n    j &lt;- k + i + 1 - N * (N - 1) / 2 + (N - i) * ((N - i) - 1) / 2;\n    i &lt;- i + 1\n    j &lt;- j + 1\n    hausdorff_distance_for(x[[i]], x[[j]])\n  })\n  attributes(out) &lt;- NULL\n  attr(out, \"Size\") &lt;- N\n  lbls &lt;- names(x)\n  attr(out, \"Labels\") &lt;- if (is.null(lbls)) 1:N else lbls\n  attr(out, \"Diag\") &lt;- FALSE\n  attr(out, \"Upper\") &lt;- FALSE\n  attr(out, \"method\") &lt;- \"hausdorff\"\n  class(out) &lt;- \"dist\"\n  out\n}"
  },
  {
    "objectID": "lab/hausdorff_dist_solutions.html#c-implementation",
    "href": "lab/hausdorff_dist_solutions.html#c-implementation",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "C++ implementation",
    "text": "C++ implementation\nThe fact that the dist function takes a list of matrices as input can be handled by {Rcpp}. However, there is no threadsafe wrapper for lists due to the fact that they can store objects of different types and thus cannot be serialized.\nWe therefore use a vector representation for an \\(L\\)-dimensional curve observed on a grid of \\(P\\) points. The vector is of length \\(L \\times N\\). This allows the entire data set to be passed as a single matrix \\(X\\) to the C++ function where the \\(i\\)-th row reads:\n\\[\nx_i^{(1)}(t_1), \\dots, x_i^{(1)}(t_P), x_i^{(2)}(t_1), \\dots, x_i^{(2)}(t_P), \\dots, x_i^{(L)}(t_1), \\dots, x_i^{(L)}(t_P).\n\\]\n\nUtility functions\nWe start by defining some utility functions that are used in the main dist() implementations.\n\n#include \"hausdorff_utils.h\"\n\ndouble hausdorff_distance_cpp(Rcpp::NumericVector x,\n                              Rcpp::NumericVector y,\n                              unsigned int dimension)\n{\n  unsigned int numDimensions = dimension;\n  unsigned int numPoints = x.size() / numDimensions;\n\n  double dX = 0.0;\n  double dY = 0.0;\n\n  for (unsigned int i = 0;i &lt; numPoints;++i)\n  {\n    double min_dist_x = std::numeric_limits&lt;double&gt;::infinity();\n    double min_dist_y = std::numeric_limits&lt;double&gt;::infinity();\n\n    for (unsigned int j = 0;j &lt; numPoints;++j)\n    {\n      double dist_x = 0.0;\n      double dist_y = 0.0;\n\n      for (unsigned int k = 0;k &lt; numDimensions;++k)\n      {\n        unsigned int index_i = k * numPoints + i;\n        unsigned int index_j = k * numPoints + j;\n        dist_x += std::pow(x[index_i] - y[index_j], 2);\n        dist_y += std::pow(y[index_i] - x[index_j], 2);\n      }\n      min_dist_x = std::min(min_dist_x, dist_x);\n      min_dist_y = std::min(min_dist_y, dist_y);\n    }\n\n    dX = std::max(dX, min_dist_x);\n    dY = std::max(dY, min_dist_y);\n  }\n\n  return std::sqrt(std::max(dX, dY));\n}\n\ndouble hausdorff_distance_cpp(RcppParallel::RMatrix&lt;double&gt;::Row x,\n                              RcppParallel::RMatrix&lt;double&gt;::Row y,\n                              unsigned int dimension)\n{\n  unsigned int numDimensions = dimension;\n  unsigned int numPoints = x.size() / numDimensions;\n\n  double dX = 0.0;\n  double dY = 0.0;\n\n  for (unsigned int i = 0;i &lt; numPoints;++i)\n  {\n    double min_dist_x = std::numeric_limits&lt;double&gt;::infinity();\n    double min_dist_y = std::numeric_limits&lt;double&gt;::infinity();\n\n    for (unsigned int j = 0;j &lt; numPoints;++j)\n    {\n      double dist_x = 0.0;\n      double dist_y = 0.0;\n\n      for (unsigned int k = 0;k &lt; numDimensions;++k)\n      {\n        unsigned index_i = k * numPoints + i;\n        unsigned index_j = k * numPoints + j;\n        dist_x += std::pow(x[index_i] - y[index_j], 2);\n        dist_y += std::pow(y[index_i] - x[index_j], 2);\n      }\n\n      min_dist_x = std::min(min_dist_x, dist_x);\n      min_dist_y = std::min(min_dist_y, dist_y);\n    }\n\n    dX = std::max(dX, min_dist_x);\n    dY = std::max(dY, min_dist_y);\n  }\n\n  return std::sqrt(std::max(dX, dY));\n}\n\nRcpp::NumericMatrix listToMatrix(Rcpp::List x)\n{\n  unsigned int nrows = x.size();\n  unsigned int ncols = Rcpp::as&lt;Rcpp::NumericVector&gt;(x[0]).size();\n  Rcpp::NumericMatrix out(nrows, ncols);\n  Rcpp::NumericMatrix workMatrix;\n  Rcpp::NumericVector workRow;\n\n  for (unsigned int i = 0;i &lt; nrows;++i)\n  {\n    workMatrix = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(x[i]);\n    workMatrix = Rcpp::transpose(workMatrix);\n    workRow = Rcpp::as&lt;Rcpp::NumericVector&gt;(workMatrix);\n    std::copy(workRow.begin(), workRow.end(), out.row(i).begin());\n  }\n\n  return out;\n}\n\nThe hausdorff_distance_cpp() function implements the Hausdorff distance between two curves, where a curve is stored as a vector as described in the beginning of the section. The function therefore takes an additional optional argument to specify the dimension of the curve. The function has two implementations with different input types (Rcpp::NumericVector and RcppParallel::RMatrix&lt;double&gt;::Row). The latter is used to parallelize the computation using the thread-safe accessor to vectors. Only the former is exported to R.\nThe listToMatrix() function is used to convert a list of matrices to a single matrix that stores the sample of curves in a format that can be passed to the dist_omp(), dist_parallel() and dist_thread() functions in a thread-safe manner.\n\n\nOpenMP implementation\n\n#include \"hausdorff_utils.h\"\n\n// // [[Rcpp::plugins(openmp)]] // Uncomment on Windows and Linux\n\nRcpp::NumericVector dist_omp(Rcpp::NumericMatrix x,\n                             unsigned int dimension = 1,\n                             unsigned int ncores = 1)\n{\n  unsigned int N = x.nrow();\n  unsigned int K = N * (N - 1) / 2;\n  Rcpp::NumericVector out(K);\n  RcppParallel::RMatrix&lt;double&gt; xSafe(x);\n  RcppParallel::RVector&lt;double&gt; outSafe(out);\n\n#ifdef _OPENMP\n#pragma omp parallel for num_threads(ncores)\n#endif\n  for (unsigned int k = 0;k &lt; K;++k)\n  {\n    unsigned int i = N - 2 - std::floor(std::sqrt(-8 * k + 4 * N * (N - 1) - 7) / 2.0 - 0.5);\n    unsigned int j = k + i + 1 - N * (N - 1) / 2 + (N - i) * ((N - i) - 1) / 2;\n    outSafe[k] = hausdorff_distance_cpp(xSafe.row(i), xSafe.row(j), dimension);\n  }\n\n  out.attr(\"Size\") = N;\n  out.attr(\"Labels\") = Rcpp::seq(1, N);\n  out.attr(\"Diag\") = false;\n  out.attr(\"Upper\") = false;\n  out.attr(\"method\") = \"hausdorff\";\n  out.attr(\"class\") = \"dist\";\n  return out;\n}\n\n// [[Rcpp::export]]\nRcpp::NumericVector dist_omp(Rcpp::List x,\n                             unsigned int dimension = 1,\n                             unsigned int ncores = 1)\n{\n  Rcpp::NumericMatrix xMatrix = listToMatrix(x);\n  return dist_omp(xMatrix, dimension, ncores);\n}\n\nThe dist_omp() function computes the Hausdorff distance between all pairs of curves in the sample. It uses OpenMP to parallelize the computation. The function has two implementations with different input types (Rcpp::NumericMatrix and Rcpp::List). The former can be wrapped with the thread-safe accessor to matrices, while the latter is the one that is exported to R and internally uses the listToMatrix() function to convert the input list into a matrix and calls the dist_omp() function with the matrix as input.\n\n\n{RcppParallel} implementation\n\n#include \"hausdorff_utils.h\"\n\nstruct HausdorffDistanceComputer : public RcppParallel::Worker\n{\n  const RcppParallel::RMatrix&lt;double&gt; m_SafeInput;\n  RcppParallel::RVector&lt;double&gt; m_SafeOutput;\n  unsigned int m_Dimension;\n\n  HausdorffDistanceComputer(const Rcpp::NumericMatrix x,\n                            Rcpp::NumericVector out,\n                            unsigned int dimension)\n    : m_SafeInput(x), m_SafeOutput(out), m_Dimension(dimension) {}\n\n  void operator()(std::size_t begin, std::size_t end)\n  {\n    unsigned int N = m_SafeInput.nrow();\n    for (std::size_t k = begin;k &lt; end;++k)\n    {\n      unsigned int i = N - 2 - std::floor(std::sqrt(-8 * k + 4 * N * (N - 1) - 7) / 2.0 - 0.5);\n      unsigned int j = k + i + 1 - N * (N - 1) / 2 + (N - i) * ((N - i) - 1) / 2;\n      m_SafeOutput[k] = hausdorff_distance_cpp(m_SafeInput.row(i), m_SafeInput.row(j), m_Dimension);\n    }\n  }\n};\n\nRcpp::NumericVector dist_parallel(Rcpp::NumericMatrix x,\n                                  unsigned int dimension = 1,\n                                  unsigned int ncores = 1)\n{\n  unsigned int N = x.nrow();\n  unsigned int K = N * (N - 1) / 2;\n  Rcpp::NumericVector out(K);\n\n  HausdorffDistanceComputer hausdorffDistance(x, out, dimension);\n  RcppParallel::parallelFor(0, K, hausdorffDistance, 1, ncores);\n\n  out.attr(\"Size\") = N;\n  out.attr(\"Labels\") = Rcpp::seq(1, N);\n  out.attr(\"Diag\") = false;\n  out.attr(\"Upper\") = false;\n  out.attr(\"method\") = \"hausdorff\";\n  out.attr(\"class\") = \"dist\";\n  return out;\n}\n\n// [[Rcpp::export]]\nRcpp::NumericVector dist_parallel(Rcpp::List x,\n                                  unsigned int dimension = 1,\n                                  unsigned int ncores = 1)\n{\n  Rcpp::NumericMatrix xMatrix = listToMatrix(x);\n  return dist_parallel(xMatrix, dimension, ncores);\n}\n\nThe dist_parallel() function computes the Hausdorff distance between all pairs of curves in the sample. It uses {RcppParallel} to parallelize the computation. The function has two implementations with different input types (Rcpp::NumericMatrix and Rcpp::List). The former can be wrapped with the thread-safe accessor to matrices, while the latter is the one that is exported to R and internally uses the listToMatrix() function to convert the input list to a matrix.\nThe dist_parallel() function parallelizes the computations via the RcppParallel::parallelFor() function, which requires a RcppParallel::Worker object. The HausdorffDistanceComputer class inherits from the RcppParallel::Worker class and is used to implement exactly what a single worker should do.\n\n\n{RcppThread} implementation\n\n#include \"hausdorff_utils.h\"\n\n// [[Rcpp::plugins(cpp11)]]\n// [[Rcpp::depends(RcppThread)]]\n#include &lt;RcppThread.h&gt;\n\nRcpp::NumericVector dist_thread(Rcpp::NumericMatrix x,\n                                unsigned int dimension = 1,\n                                unsigned int ncores = 1)\n{\n  unsigned int N = x.nrow();\n  unsigned int K = N * (N - 1) / 2;\n  Rcpp::NumericVector out(K);\n  RcppParallel::RMatrix&lt;double&gt; xSafe(x);\n  RcppParallel::RVector&lt;double&gt; outSafe(out);\n\n  auto task = [&xSafe, &outSafe, &dimension] (unsigned int k) {\n    unsigned int N = xSafe.nrow();\n    unsigned int i = N - 2 - std::floor(std::sqrt(-8 * k + 4 * N * (N - 1) - 7) / 2.0 - 0.5);\n    unsigned int j = k + i + 1 - N * (N - 1) / 2 + (N - i) * ((N - i) - 1) / 2;\n    outSafe[k] = hausdorff_distance_cpp(xSafe.row(i), xSafe.row(j), dimension);\n  };\n\n  RcppThread::parallelFor(0, K, task, ncores);\n\n  out.attr(\"Size\") = N;\n  out.attr(\"Labels\") = Rcpp::seq(1, N);\n  out.attr(\"Diag\") = false;\n  out.attr(\"Upper\") = false;\n  out.attr(\"method\") = \"hausdorff\";\n  out.attr(\"class\") = \"dist\";\n  return out;\n}\n\n// [[Rcpp::export]]\nRcpp::NumericVector dist_thread(Rcpp::List x,\n                                unsigned int dimension = 1,\n                                unsigned int ncores = 1)\n{\n  Rcpp::NumericMatrix xMatrix = listToMatrix(x);\n  return dist_thread(xMatrix, dimension, ncores);\n}\n\nThe dist_thread() function computes the Hausdorff distance between all pairs of curves in the sample. It uses {RcppThread} to parallelize the computation. The function has two implementations with different input types (Rcpp::NumericMatrix and Rcpp::List). The former can be wrapped with the thread-safe accessor to matrices, while the latter is the one that is exported to R and internally uses the listToMatrix() function to convert the input list to a matrix.\nThe dist_thread() function parallelizes the computations via the RcppThread::parallelFor() function, which requires a task to be defined to tell each worker exactly what to do. This is achieved using a lambda function which is a C++ feature available since the C++11 standard."
  },
  {
    "objectID": "lab/hausdorff_dist_solutions.html#benchmark",
    "href": "lab/hausdorff_dist_solutions.html#benchmark",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "Benchmark",
    "text": "Benchmark\n\nlibrary(future)\n\nbm &lt;- bench::mark(\n  sequential = dist_r_v1(dat, vectorized = FALSE),\n  outer = {\n    plan(multisession, workers = 4L)\n    out &lt;- dist_r_v2(dat)\n    plan(sequential)\n    out\n  },\n  chunksize = {\n    plan(multisession, workers = 4L)\n    out &lt;- dist_r_v3(dat)\n    plan(sequential)\n    out\n  },\n  nested = {\n    plan(list(\n      tweak(multisession, workers = 2L),\n      tweak(multisession, workers = I(2L))\n    ))\n    out &lt;- dist_r_v4(dat)\n    plan(sequential)\n    out\n  },\n  singleloop = {\n    plan(multisession, workers = 4L)\n    out &lt;- dist_r_v5(dat)\n    plan(sequential)\n    out\n  },\n  omp1 = dist_omp(dat, dimension = 3L, ncores = 1L),\n  omp4 = dist_omp(dat, dimension = 3L, ncores = 4L),\n  parallel1 = dist_parallel(dat, dimension = 3L, ncores = 1L),\n  parallel4 = dist_parallel(dat, dimension = 3L, ncores = 4L),\n  thread1 = dist_thread(dat, dimension = 3L, ncores = 1L),\n  thread4 = dist_thread(dat, dimension = 3L, ncores = 4L)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of different implementations\n\n\nThe computation time is given in milliseconds and the memory allocation in bytes.\n\n\nLanguage\nExpression\nMedian computation time\nMemory allocation\n\n\n\n\nR\nsequential\n11,342.61 ms\n  250.3 kB\n\n\nR\nouter\n 5,247.86 ms\n13 MB \n\n\nR\nchunksize\n 3,687.32 ms\n   27.6 MB\n\n\nR\nnested\n 4,430.23 ms\n   17.4 MB\n\n\nR\nsingleloop\n 3,714.72 ms\n 7 MB \n\n\nC++\nomp1\n    25.91 ms\n  204.4 kB\n\n\nC++\nomp4\n     6.83 ms\n  209.1 kB\n\n\nC++\nparallel1\n    25.98 ms\n  204.4 kB\n\n\nC++\nparallel4\n     6.87 ms\n  204.4 kB\n\n\nC++\nthread1\n    25.92 ms\n  204.4 kB\n\n\nC++\nthread4\n     6.77 ms\n  204.4 kB"
  },
  {
    "objectID": "lab/hausdorff_dist_solutions.html#interpretation",
    "href": "lab/hausdorff_dist_solutions.html#interpretation",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "Interpretation",
    "text": "Interpretation\nGeneral observation. Using C++ is much faster than using R. This is in particular due to never copying the data. It also provides linear speed-up as expected. There are no notable differences between the different C++ implementations.\nR implementation. The original Hausdorff distance implementation has a double loop.\n\nThe outer loop generates unbalanced chunks, which is not optimal (outer entry).\nLoad balancing is better with a fixed chunk size of 1 (chunksize entry).\nUsing nested parallelization does not help (nested entry) achieves intermediate results. It is better than the outer loop but worse than the fixed chunk size.\nTransforming the original double loop into a single loop (singleloop entry) is as fast as parallelizing the outer loop using a chunk size of 1 (singleloop entry) but its memory allocation is much lower. This is because, when the chunk size is 1, a future is created for each iteration of the loop, and the whole data is copied for each future. In contrast, the single loop version submits a single future per worker which creates less copies of the data.\nIn general, playing with chunk size and nested parallelization generates more future objects and copies the data more often, which increases memory allocation."
  },
  {
    "objectID": "lab/hausdorff_dist.html",
    "href": "lab/hausdorff_dist.html",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "",
    "text": "We have simulated 3D functional data for this lab that is provided in the Quarto document in the dat object.\nThe dat object is a list of size \\(100\\) containing \\(100\\) three-dimensional curves observed on a common grid of size \\(200\\) of the interval \\([0, 1]\\).\nAs a result, each element of the dat list is a \\(3 \\times 200\\) matrix.\nHere we focus on a subset of the data, the first \\(21\\) curves, which looks like:\n\nplot(mfdat)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObjective\n\n\n\nThe goal is to implement a function similar to stats::dist() which computes the pairwise distance matrix on this functional dataset using the Hausdorff distance."
  },
  {
    "objectID": "lab/hausdorff_dist.html#data-goal",
    "href": "lab/hausdorff_dist.html#data-goal",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "",
    "text": "We have simulated 3D functional data for this lab that is provided in the Quarto document in the dat object.\nThe dat object is a list of size \\(100\\) containing \\(100\\) three-dimensional curves observed on a common grid of size \\(200\\) of the interval \\([0, 1]\\).\nAs a result, each element of the dat list is a \\(3 \\times 200\\) matrix.\nHere we focus on a subset of the data, the first \\(21\\) curves, which looks like:\n\nplot(mfdat)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObjective\n\n\n\nThe goal is to implement a function similar to stats::dist() which computes the pairwise distance matrix on this functional dataset using the Hausdorff distance."
  },
  {
    "objectID": "lab/hausdorff_dist.html#hausdorff-distance-in-r",
    "href": "lab/hausdorff_dist.html#hausdorff-distance-in-r",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "Hausdorff distance in R",
    "text": "Hausdorff distance in R\nWe can implement the Hausdorff distance between two curves as:\n\nhausdorff_distance_vec &lt;- function(x, y) {\n  P &lt;- ncol(x)\n  dX &lt;- 1:P |&gt;\n    purrr::map_dbl(\\(p) {\n      min(colSums((y - x[, p])^2))\n    }) |&gt;\n    max()\n  dY &lt;- 1:P |&gt;\n    purrr::map_dbl(\\(p) {\n      min(colSums((x - y[, p])^2))\n    }) |&gt;\n    max()\n  sqrt(max(dX, dY))\n}\n\nThis version exploits the vectorized nature of R to compute the Hausdorff distance via calls to colSums() and max(). Another version based on a double loop is provided by the following hausdorff_distance_for() function:\n\nhausdorff_distance_for &lt;- function(x, y) {\n  P &lt;- ncol(x)\n  dX &lt;- 0\n  dY &lt;- 0\n  for (i in 1:P) {\n    min_dist_x &lt;- Inf\n    min_dist_y &lt;- Inf\n    for (j in 1:P) {\n      dist_x &lt;- sum((y[, j] - x[, i])^2)\n      if (dist_x &lt; min_dist_x) {\n        min_dist_x &lt;- dist_x\n      }\n      dist_y &lt;- sum((x[, j] - y[, i])^2)\n      if (dist_y &lt; min_dist_y) {\n        min_dist_y &lt;- dist_y\n      }\n    }\n    if (min_dist_x &gt; dX) {\n      dX &lt;- min_dist_x\n    }\n    if (min_dist_y &gt; dY) {\n      dY &lt;- min_dist_y\n    }\n  }\n  sqrt(max(dX, dY))\n}\n\nWe can benchmark the two versions:\n\nbm &lt;- bench::mark(\n  hausdorff_distance_vec(dat[[1]], dat[[2]]),\n  hausdorff_distance_for(dat[[1]], dat[[2]])\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpression\nMedian computation time\nMemory allocation\n\n\n\n\nhausdorff_distance_vec(dat[[1]], dat[[2]])\n 2.02 ms\n  2.7 MB\n\n\nhausdorff_distance_for(dat[[1]], dat[[2]])\n50.08 ms\n124.6 kB\n\n\n\n\n\n\n\nWe conclude that the vectorized version is faster but has a huge memory footprint compared to the loop-based version. This means that the vectorized version is not suitable for even moderately large data sets."
  },
  {
    "objectID": "lab/hausdorff_dist.html#pairwise-distance-matrix-in-r",
    "href": "lab/hausdorff_dist.html#pairwise-distance-matrix-in-r",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "Pairwise distance matrix in R",
    "text": "Pairwise distance matrix in R\n\n\n\n\n\n\ndist objects\n\n\n\nTake a look at the documentation of the stats::dist() function to understand how to make an object of class dist.\n\n\nWe can exploit the previous functions to compute the pairwise distance matrix using the Hausdorff distance:\n\ndist_r_v1 &lt;- function(x, vectorized = FALSE) {\n  hausdorff_distance &lt;- if (vectorized) \n    hausdorff_distance_vec\n  else \n    hausdorff_distance_for\n  N &lt;- length(x)\n  out &lt;- 1:(N - 1) |&gt;\n    purrr::map(\\(i) {\n      purrr::map_dbl((i + 1):N, \\(j) {\n        hausdorff_distance(x[[i]], x[[j]])\n      })\n    }) |&gt;\n    purrr::list_c()\n\n  attributes(out) &lt;- NULL\n  attr(out, \"Size\") &lt;- N\n  lbls &lt;- names(x)\n  attr(out, \"Labels\") &lt;- if (is.null(lbls)) 1:N else lbls\n  attr(out, \"Diag\") &lt;- FALSE\n  attr(out, \"Upper\") &lt;- FALSE\n  attr(out, \"method\") &lt;- \"hausdorff\"\n  class(out) &lt;- \"dist\"\n  out\n}\n\nWe can benchmark the two versions:\n\nbm &lt;- bench::mark(\n  dist_r_v1(dat, vectorized = TRUE),\n  dist_r_v1(dat, vectorized = FALSE)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpression\nMedian computation time\nMemory allocation\n\n\n\n\ndist_r_v1(dat, vectorized = TRUE)\n 0.48 s\n 546.5 MB\n\n\ndist_r_v1(dat, vectorized = FALSE)\n11.18 s\n3 kB \n\n\n\n\n\n\n\n\n\n\n\n\n\nMemory footprint\n\n\n\nWe confirm that the vectorized version is not scalable to large datasets. Using it on the full dataset actually requires 12GB of memory! We will therefore focus on the loop-based version from now on."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Instructions for the tutorial",
    "section": "",
    "text": "Material for the Parallel Rcpp tutorial at the ANF High Performance Computing with R, Frejus, France.\nTotal time: 3 hours (tentatively).\nThe main webpage is at https://astamm.github.io/parallel-rcpp/."
  },
  {
    "objectID": "index.html#material",
    "href": "index.html#material",
    "title": "Instructions for the tutorial",
    "section": "",
    "text": "Material for the Parallel Rcpp tutorial at the ANF High Performance Computing with R, Frejus, France.\nTotal time: 3 hours (tentatively).\nThe main webpage is at https://astamm.github.io/parallel-rcpp/."
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "Instructions for the tutorial",
    "section": "Requirements",
    "text": "Requirements\n\nR: https://www.r-project.org\nRStudio: https://posit.co/download/rstudio-desktop/\nQuarto: https://quarto.org/docs/get-started/\nQuarto Drop extension: https://github.com/r-wasm/quarto-drop\n[MacOS only] OpenMP support: https://mac.r-project.org/openmp/\nR packages:\n\n{bench}\n{BH}\n{futureverse}\n{gt}\n{Rcpp}\n{RcppParallel}\n{RcppProgress}\n{RcppThread}\n{roahd}\n{sitmo}\n{tidyverse}"
  }
]